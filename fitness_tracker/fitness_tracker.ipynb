{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e5fe90",
   "metadata": {},
   "source": [
    "# Predicting the amount of sleep\n",
    "In the following we will be using a [Kaggle dataset for the bellabeaat fitness tracker](https://www.kaggle.com/blessingalabie/bellabeaat-fitness-tracker-device-data?select=hourlyCalories_merged.csv) to try to predict the hours of sleep for a person on a given day.\n",
    "\n",
    "The dataset contains three tables, which contain the following data:\n",
    "1. A table which has hourly data on burned calories for a personId. That is every datapoint consists of `(personId, hourlyTimestamp, caloriesBurnt)`\n",
    "2. An activity data table, which contains datapoints of the form `(personId, hourlyTimestamp, totalIntensity, averageIntensity)`\n",
    "3. Sleep data aggreagted in a table containing of the form `(personId, sleepDay, totalSleepRecords, totalMinutesAsleep, totalTimeInBed)`\n",
    "\n",
    "## Preprocessing\n",
    "In order to make predictions about the amount of sleep we first have to aggregate the three tables into one usable table. We try the following strategy:\n",
    "1. Take the sleep data as-is\n",
    "2. We sum up the calories per day and then include them in the table \n",
    "3. We sum up the total intensity of the exercise on a given day and include it.\n",
    "\n",
    "Note the following subtetly: All sleep records are set at 12am for the whole day. Given that sleep patterns can vary over the day we will include points 2./3. for the day before the sleep datapoint as well as the day itself to make sure that all information is contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccba4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc531a",
   "metadata": {},
   "source": [
    "Let's load the three datasets and bring them into the above mentioned form.\n",
    "(Uncomment any of the lines below to look at the raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc35d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_calories = pd.read_csv(\"fitness_tracker/hourlyCalories_merged.csv\")\n",
    "hourly_intensities = pd.read_csv(\"fitness_tracker/hourlyIntensities_merged.csv\")\n",
    "sleep_data = pd.read_csv(\"fitness_tracker/sleepDay_merged.csv\")\n",
    "#hourly_calories.head()\n",
    "#hourly_intensities.head()\n",
    "#sleep_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f722d",
   "metadata": {},
   "source": [
    "We bring the data to the above mentioned form\n",
    "1. Drop the time from the datetime-stamps in all three tables\n",
    "2. Aggregate over day+personId for the calories- and activity tables\n",
    "3. Join the tables to one big table\n",
    "4. Look for empty fields and consider dropping those lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbec3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(datetime_string):\n",
    "    return datetime_string.split(\" \")[0]\n",
    "\n",
    "def get_daily_df(df, sum_column):\n",
    "    return df.drop(columns=[\"ActivityHour\"])\\\n",
    "        .groupby([\"Id\", \"ActivityDate\"])[sum_column]\\\n",
    "        .sum()\\\n",
    "        .reset_index()\n",
    "\n",
    "def get_merged_df(sleep_data, daily_calories, daily_intensities):\n",
    "    return sleep_data.drop(columns=[\"SleepDay\"])\\\n",
    "            .merge(daily_calories, left_on=[\"Id\", \"ActivityDate\"], right_on=[\"Id\", \"ActivityDate\"])\\\n",
    "            .merge(daily_intensities, left_on=[\"Id\", \"ActivityDate\"], right_on=[\"Id\", \"ActivityDate\"])\n",
    "\n",
    "hourly_calories[\"ActivityDate\"] = hourly_calories.ActivityHour.apply(get_date)\n",
    "hourly_intensities[\"ActivityDate\"] = hourly_intensities.ActivityHour.apply(get_date)\n",
    "sleep_data[\"ActivityDate\"] = sleep_data.SleepDay.apply(get_date)\n",
    "\n",
    "daily_calories = get_daily_df(hourly_calories, \"Calories\")\n",
    "daily_intensities = get_daily_df(hourly_intensities, \"TotalIntensity\")\n",
    "\n",
    "aggregated_df = get_merged_df(sleep_data, daily_calories, daily_intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4662dd2",
   "metadata": {},
   "source": [
    "## Data quality\n",
    "In total we end up with 412 rows of data. Since, for simplification, we used an inner-join to merge the data from the three tables there are no missing values we have to take care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76376a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique persons in data set:  24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>TotalSleepRecords</th>\n",
       "      <th>TotalMinutesAsleep</th>\n",
       "      <th>TotalTimeInBed</th>\n",
       "      <th>ActivityDate</th>\n",
       "      <th>Calories</th>\n",
       "      <th>TotalIntensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>1</td>\n",
       "      <td>327</td>\n",
       "      <td>346</td>\n",
       "      <td>4/12/2016</td>\n",
       "      <td>1988</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>2</td>\n",
       "      <td>384</td>\n",
       "      <td>407</td>\n",
       "      <td>4/13/2016</td>\n",
       "      <td>1798</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>1</td>\n",
       "      <td>412</td>\n",
       "      <td>442</td>\n",
       "      <td>4/15/2016</td>\n",
       "      <td>1745</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>2</td>\n",
       "      <td>340</td>\n",
       "      <td>367</td>\n",
       "      <td>4/16/2016</td>\n",
       "      <td>1866</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1503960366</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>712</td>\n",
       "      <td>4/17/2016</td>\n",
       "      <td>1730</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>8792009665</td>\n",
       "      <td>1</td>\n",
       "      <td>343</td>\n",
       "      <td>360</td>\n",
       "      <td>4/30/2016</td>\n",
       "      <td>2897</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>8792009665</td>\n",
       "      <td>1</td>\n",
       "      <td>503</td>\n",
       "      <td>527</td>\n",
       "      <td>5/1/2016</td>\n",
       "      <td>1963</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>8792009665</td>\n",
       "      <td>1</td>\n",
       "      <td>415</td>\n",
       "      <td>423</td>\n",
       "      <td>5/2/2016</td>\n",
       "      <td>2013</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>8792009665</td>\n",
       "      <td>1</td>\n",
       "      <td>516</td>\n",
       "      <td>545</td>\n",
       "      <td>5/3/2016</td>\n",
       "      <td>2298</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>8792009665</td>\n",
       "      <td>1</td>\n",
       "      <td>439</td>\n",
       "      <td>463</td>\n",
       "      <td>5/4/2016</td>\n",
       "      <td>2065</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  TotalSleepRecords  TotalMinutesAsleep  TotalTimeInBed  \\\n",
       "0    1503960366                  1                 327             346   \n",
       "1    1503960366                  2                 384             407   \n",
       "2    1503960366                  1                 412             442   \n",
       "3    1503960366                  2                 340             367   \n",
       "4    1503960366                  1                 700             712   \n",
       "..          ...                ...                 ...             ...   \n",
       "408  8792009665                  1                 343             360   \n",
       "409  8792009665                  1                 503             527   \n",
       "410  8792009665                  1                 415             423   \n",
       "411  8792009665                  1                 516             545   \n",
       "412  8792009665                  1                 439             463   \n",
       "\n",
       "    ActivityDate  Calories  TotalIntensity  \n",
       "0      4/12/2016      1988             429  \n",
       "1      4/13/2016      1798             318  \n",
       "2      4/15/2016      1745             364  \n",
       "3      4/16/2016      1866             349  \n",
       "4      4/17/2016      1730             318  \n",
       "..           ...       ...             ...  \n",
       "408    4/30/2016      2897             371  \n",
       "409     5/1/2016      1963              79  \n",
       "410     5/2/2016      2013             101  \n",
       "411     5/3/2016      2298             156  \n",
       "412     5/4/2016      2065             129  \n",
       "\n",
       "[413 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personsInDataSet = len(aggregated_df.Id.unique())\n",
    "print(\"Unique persons in data set: \", personsInDataSet)\n",
    "\n",
    "#aggregated_df.describe()\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb049c40",
   "metadata": {},
   "source": [
    "## Defining the ML problem\n",
    "As discussed before, we want to use this data to predict the total time a person slept on a given day using the other columns. To do so, we first need to define what input we use and turn that input into a suitable form for the ML algorithm.\n",
    "\n",
    "We suspect a strong correlation of total sleep to an individual person, which is why we need to turn the personId's into a form that is readible for the ML-algorithm. We do so by converting it to a one-hot vector of the 24 persons in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e9aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = pd.get_dummies(aggregated_df.Id, prefix='personId')\n",
    "one_hot_dataframe = aggregated_df.join(one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af9552",
   "metadata": {},
   "source": [
    "As a simplifiying assumption (see discussion at end), we will look at the rows as individual datapoints even though we suspect correlations over time. This means we drop the `ActivityDate` column for now. (We also drop the `Id` columns as we have replace it by the one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca72ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_input = one_hot_dataframe.drop(columns=[\"Id\", \"ActivityDate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bc870",
   "metadata": {},
   "source": [
    "Finally, we want to split the data set into a portion we use for training and a smaller portion we use for testing. This should be done in a random, but reproduceable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07086eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 371  and testing set:  42\n"
     ]
    }
   ],
   "source": [
    "random_state = 27\n",
    "train_df, test_df = train_test_split(ml_input, test_size=0.1, random_state=random_state)\n",
    "print(\"Size of training set:\", len(train_df), \" and testing set: \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc8931",
   "metadata": {},
   "source": [
    "## The ML-algorithm\n",
    "Let us apply a basic neural net to this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b05e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 14:00:04.182544: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "no_of_input_columns = len(ml_input.columns) - 1\n",
    "no_of_dense_layers = 3\n",
    "dense_layer_units = 32\n",
    "dropout_rate = 0.1\n",
    "learning_rate = ExponentialDecay(initial_learning_rate=0.01, decay_rate=0.9999, decay_steps=1)\n",
    "\n",
    "initializer = RandomNormal(stddev=0.1)\n",
    "\n",
    "input_layer = Input(shape=(no_of_input_columns,), dtype='float32')\n",
    "neural_net_layers = [input_layer]\n",
    "for i in range(no_of_dense_layers):\n",
    "    layer = Dense(units=dense_layer_units, activation='relu',\\\n",
    "                  kernel_initializer=initializer)(neural_net_layers[-1])\n",
    "    neural_net_layers.append(layer)\n",
    "    \n",
    "neural_net_layers.append(Dropout(rate=dropout_rate)(neural_net_layers[-1]))\n",
    "output_layer = Dense(units=1, activation='relu')(neural_net_layers[-1])\n",
    "    \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2dd3e",
   "metadata": {},
   "source": [
    "## Training the algorithm\n",
    "We first split the training set into input and and output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904929d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_and_output(df):\n",
    "    nn_input = df.drop(columns=[\"TotalMinutesAsleep\"]).to_numpy()\n",
    "    nn_output = df.TotalMinutesAsleep.to_numpy()\n",
    "    return(nn_input, nn_output)\n",
    "    \n",
    "(nn_input, nn_output) = get_input_and_output(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b06d",
   "metadata": {},
   "source": [
    "We configure to save the weights to a file in between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b1d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"fitness_tracker.hdf5\"\n",
    "callbacks_list = [ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True,\\\n",
    "                                  save_weights_only=False, mode='auto', save_freq=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fea93",
   "metadata": {},
   "source": [
    "Finally, we are ready to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbed418",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 3000\n",
    "\n",
    "model.fit(x=nn_input, y=nn_output, batch_size=batch_size, epochs=epochs,\\\n",
    "          callbacks=callbacks_list, verbose=0);\n",
    "model.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cf110",
   "metadata": {},
   "source": [
    "# Some notes on the choice of hyperparameters\n",
    "\n",
    "Given the small size of the model, we can iterate over many points in hyperparameter space, such that we could find suitable values for the number of layers, their size, the number of epochs to run and the size of batches to be processed at the same point quickly.\n",
    "\n",
    "Some problems we faced when training the model where:\n",
    "1. Instability: Running the model repeatedly with the same parameters did not always result in the fit to converge. Outside of the randomness of the initial weights, this depends largely on how high the initial value of the learning rate is. Moreover, introducing a Dropout improved this.\n",
    "2. The loss function does not converge properly over time, but rather fluctuates. We addressed this successfully by introducing an exponential decay in the learning rate.\n",
    "3. Overfitting: This was the actual reason to introduce the Dropout mentioned in 1. it completely solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ec452",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "Let us briefly calculate the loss our model has on predictions on the test set:\n",
    "\n",
    "(You can comment out the line to load the pre-trained weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6de5022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 302.2112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "302.2112121582031"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_weights(\"fitness_tracker_demo.hdf5\")\n",
    "(test_input, test_output) = get_input_and_output(test_df)\n",
    "model.evaluate(test_input, test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fd85b",
   "metadata": {},
   "source": [
    "By running the model 10 times we got an average loss of about 317.\n",
    "\n",
    "The loss is not a very intutive measure, however. So let us instead construct calculate the relative error of the prediction over the test data set, to get a better understanding on how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cca1427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>RelativeError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>523</td>\n",
       "      <td>506.0</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>651</td>\n",
       "      <td>618.0</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>329</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>443</td>\n",
       "      <td>436.0</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340</td>\n",
       "      <td>338.0</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>235</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>436</td>\n",
       "      <td>445.0</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>543</td>\n",
       "      <td>552.0</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>531</td>\n",
       "      <td>524.0</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>409</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>250</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>528</td>\n",
       "      <td>509.0</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>237</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>478</td>\n",
       "      <td>471.0</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>447</td>\n",
       "      <td>443.0</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>594</td>\n",
       "      <td>562.0</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>527</td>\n",
       "      <td>506.0</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>503</td>\n",
       "      <td>508.0</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>527</td>\n",
       "      <td>504.0</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>364</td>\n",
       "      <td>364.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>457</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>396</td>\n",
       "      <td>380.0</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>450</td>\n",
       "      <td>459.0</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>465</td>\n",
       "      <td>440.0</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>477</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>469</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>459</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>402</td>\n",
       "      <td>395.0</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>644</td>\n",
       "      <td>659.0</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>603</td>\n",
       "      <td>581.0</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>476</td>\n",
       "      <td>508.0</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>377</td>\n",
       "      <td>375.0</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>323</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>611</td>\n",
       "      <td>617.0</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>529</td>\n",
       "      <td>495.0</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>552</td>\n",
       "      <td>544.0</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>418</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>455</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>479</td>\n",
       "      <td>466.0</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>62</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>545</td>\n",
       "      <td>526.0</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Real  Predicted  RelativeError\n",
       "0    523      506.0          0.034\n",
       "1    651      618.0          0.053\n",
       "2    329      319.0          0.031\n",
       "3    443      436.0          0.016\n",
       "4    340      338.0          0.006\n",
       "5    106       91.0          0.165\n",
       "6    235      250.0          0.060\n",
       "7    436      445.0          0.020\n",
       "8    543      552.0          0.016\n",
       "9    531      524.0          0.013\n",
       "10   409      399.0          0.025\n",
       "11   250      228.0          0.096\n",
       "12   528      509.0          0.037\n",
       "13   237      277.0          0.144\n",
       "14   478      471.0          0.015\n",
       "15   447      443.0          0.009\n",
       "16   594      562.0          0.057\n",
       "17   527      506.0          0.042\n",
       "18   503      508.0          0.010\n",
       "19   527      504.0          0.046\n",
       "20   364      364.0          0.000\n",
       "21   457      452.0          0.011\n",
       "22   396      380.0          0.042\n",
       "23   450      459.0          0.020\n",
       "24   465      440.0          0.057\n",
       "25   477      458.0          0.041\n",
       "26   469      450.0          0.042\n",
       "27   459      458.0          0.002\n",
       "28   402      395.0          0.018\n",
       "29   644      659.0          0.023\n",
       "30   603      581.0          0.038\n",
       "31   476      508.0          0.063\n",
       "32   377      375.0          0.005\n",
       "33   323      322.0          0.003\n",
       "34   611      617.0          0.010\n",
       "35   529      495.0          0.069\n",
       "36   552      544.0          0.015\n",
       "37   418      400.0          0.045\n",
       "38   455      442.0          0.029\n",
       "39   479      466.0          0.028\n",
       "40    62       72.0          0.139\n",
       "41   545      526.0          0.036"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_error = lambda real, predicted: np.round(abs(real/predicted - 1.0), 3)\n",
    "\n",
    "prediction = np.round(model.predict(test_input).flatten(),0)\n",
    "rel_error(test_output, prediction)\n",
    "evaluation_df = pd.DataFrame({\"Real\": test_output,\\\n",
    "                             \"Predicted\": prediction,\\\n",
    "                             \"RelativeError\": rel_error(test_output, prediction)})\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff6595b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>RelativeError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>444.047619</td>\n",
       "      <td>436.880951</td>\n",
       "      <td>0.038833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>128.935029</td>\n",
       "      <td>124.806427</td>\n",
       "      <td>0.037497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>397.500000</td>\n",
       "      <td>383.750000</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>462.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>527.750000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>0.045750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>651.000000</td>\n",
       "      <td>659.000000</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Real   Predicted  RelativeError\n",
       "count   42.000000   42.000000      42.000000\n",
       "mean   444.047619  436.880951       0.038833\n",
       "std    128.935029  124.806427       0.037497\n",
       "min     62.000000   72.000000       0.000000\n",
       "25%    397.500000  383.750000       0.015000\n",
       "50%    462.000000  455.000000       0.030000\n",
       "75%    527.750000  508.000000       0.045750\n",
       "max    651.000000  659.000000       0.165000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b931130",
   "metadata": {},
   "source": [
    "We can see that the average relative error lies around 4%. Given that this is a very small dataset (412 entries), this is a comparably precise prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f2862",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "Given the different columns clearly there is the suspicion that certain features, such as `TotalTimeInBed` are a very good predictor even without considering other parameters. Let us check this here.\n",
    "\n",
    "We do so, by randomizing one feature over the test set and then compare the prediction for this modified set with the original one.\n",
    "\n",
    "The output of the following code snippet is a list of the loss we get for our model when we shuffle a given feature. That is, if the value is very high, this means that the importance of the feature is very high.\n",
    "\n",
    "As expected the model depends very strongly on the variable `TotalTimeInBed`. To construct a model without this feature would likely require more data and some changes to the model (see below for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7c6a16e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TotalSleepRecords        304.485870\n",
       "TotalTimeInBed         32156.130859\n",
       "Calories                 626.766724\n",
       "TotalIntensity           384.527710\n",
       "personId_1503960366      448.205139\n",
       "personId_1644430081      302.211212\n",
       "personId_1844505072      879.057739\n",
       "personId_1927972279      302.211212\n",
       "personId_2026352035      350.017365\n",
       "personId_2320127002      302.211212\n",
       "personId_2347167796      303.120026\n",
       "personId_3977333714      608.467896\n",
       "personId_4020332650      307.493591\n",
       "personId_4319703577      451.776825\n",
       "personId_4388161847      306.835724\n",
       "personId_4445114986      303.662537\n",
       "personId_4558609924      302.211212\n",
       "personId_4702921684      303.228424\n",
       "personId_5553957443      302.406860\n",
       "personId_5577150313      308.495544\n",
       "personId_6117666160      302.211212\n",
       "personId_6775888955      290.321564\n",
       "personId_6962181067      325.307892\n",
       "personId_7007744171      302.211212\n",
       "personId_7086361926      315.425262\n",
       "personId_8053475328      302.211212\n",
       "personId_8378563200      303.647705\n",
       "personId_8792009665      306.626221\n",
       "dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_randomized_feature_df(df, feature):\n",
    "    new_df = df.copy(deep=True)\n",
    "    new_df[feature] = df[feature].sample(frac=1).values\n",
    "    return new_df\n",
    "\n",
    "def get_feature_importance(df, feature):\n",
    "    shuffled_feature_df = get_randomized_feature_df(df, feature)\n",
    "    (test_input, test_output) = get_input_and_output(shuffled_feature_df)\n",
    "    return model.evaluate(test_input, test_output, return_dict=True, verbose=0)['loss']\n",
    "\n",
    "a = get_feature_importance(test_df, \"TotalTimeInBed\")\n",
    "test = lambda feature: get_feature_importance(test_df, feature)\n",
    "test_df.drop(columns=[\"TotalMinutesAsleep\"]).columns.to_series().apply(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282a554",
   "metadata": {},
   "source": [
    "## Potential improvements\n",
    "The way how we preprocessed our dataset completely neglected the time-series nature of it. Clearly, it is somewhat expected that there exist correlations over time (e.g. calories burnt on the days before a sleep sessions might still be a good predictor). One could try to incorporate this either simply by adding another column like `caloriesBurntOnDayBefore`, or one could use neural network architectures specialized for time series data.\n",
    "\n",
    "Given the small size of the dataset however, we would likely not have enough data to test and train this.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

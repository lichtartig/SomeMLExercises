{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6a305c",
   "metadata": {},
   "source": [
    "# Q-Learning algorithm for cart-pole\n",
    "\n",
    "The following is an exercise in reinforcement learning. We consider a basic problem from the `OpenAI-Gym`. In which we use a Q-Learner algorithm to have a neural network converge to a state in which it can balance a stick on a moveable cart.\n",
    "\n",
    "At any given point in time the state of the system is defined by its position and the angle of the stick. If the angle of the stick is too big, the stick falls. To keep that from happening an action can be performed, that is the Q-Learner can move the cart left or right with a given velocity. (See below for a graphical representation of this)\n",
    "\n",
    "**Q-Learning** means that the input of the neural network is a state combined with a possible action to take in that state. (That is the position of the cartpole combined with the action to move it left or right -- we neglect its velocity for simplicity). The output of the neural network is an value estimating how good that action is (Q-value).\n",
    "\n",
    "**We train the neural network by** using its own training episodes in which we approximate the Q-value by taking it dependent on how long it manages to maintain the stick balanced (see below).\n",
    "\n",
    "Once the algorithm is trained, one can select an action by selecting the action with the highest Q-value or by treating them as probabilities to introduce some randomness.\n",
    "\n",
    "We will use the `tensorflow.Keras`-framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63838729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from _collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e12ae2",
   "metadata": {},
   "source": [
    "The environment can be set up by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd8f65",
   "metadata": {},
   "source": [
    "Let us fix some parameters before we begin. The two most interesting ones are:\n",
    "1. `relative_randomness`\n",
    "2. `randomness_decay`\n",
    "The reason behind these parameters is that an initial state of a neural network might include some initial bias towards certain actions that keep it from generating unbiased testing date.\n",
    "So we start out by selecting random moves, over time the neural network starts converging towards a state in which its actions are better, such that we take less and less random moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0be4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_capacity = 1000\n",
    "episodes_until_abortion = 10000\n",
    "relative_randomness = 1\n",
    "randomness_decay = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cc0f1",
   "metadata": {},
   "source": [
    "Let us define a utility class which holds an *experience buffer* with a maximum capacity specified by 'capacity' to which we append training episodes. We can then retrieve randomized batches of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.buffer = deque()\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def save_step(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.popleft()\n",
    "\n",
    "    def get_experience_batch(self):\n",
    "        return random.sample(self.buffer, min(len(self.buffer), self.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80cbe1",
   "metadata": {},
   "source": [
    "We initialize it using the parameters specified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_buffer = ExperienceBuffer(batch_size=batch_size, capacity=buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8ed90",
   "metadata": {},
   "source": [
    "## The Q-Learning algorithm:\n",
    "We encapsulate the Q-Learning algorithm in a class:\n",
    "* The constructor contains the topology of the neural network and some hyperparameters:\n",
    "    1. We use three linear layers with a `relu`-activation layer the last of which has the output-shape of just one number: The Q-value.\n",
    "    2. The learning rate decays exponentially\n",
    "    3. We use the `Adam`-optimizer with mean squared errors as the loss function.\n",
    "    4. Finally, we specify a parameter `gamma`, which controls at what step number we expect the outcome to be correlated with an action (i.e. a positive/negative outcome should not affect our estimate for the reward of an action taken many steps before)\n",
    "* The `preprocess` function turns a list of experience tuples `(state, action, reward, next_state)` into inputs and outputs for the neural net.\n",
    "* the `get_action` function returns an action based upon an input state either using the neural net or randomly (if the randomness `eps` is non-zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1368aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    worst_reward = -10\n",
    "    all_actions = np.array([0, 1])\n",
    "    gamma = 0.75\n",
    "    dense_layer_size = 24\n",
    "    normalization = [0.42, 1]\n",
    "    \n",
    "    def __init__(self, weight_file):\n",
    "        self.weight_file = weight_file\n",
    "        self.glue = lambda states, actions: np.concatenate((states[:, 2:] / self.normalization,\n",
    "                                                            actions[:, None]), axis=1)\n",
    "\n",
    "        initializer = RandomNormal(stddev=0.1)\n",
    "        input_layer = Input(shape=(3,), dtype='float32')\n",
    "        layers = [input_layer]\n",
    "        layers.append(Dense(units=self.dense_layer_size, activation='relu', kernel_initializer=initializer)(layers[-1]))\n",
    "        layers.append(Dense(units=self.dense_layer_size, activation='relu', kernel_initializer=initializer)(layers[-1]))\n",
    "\n",
    "        output_layer = Dense(units=1, activation='relu')(layers[-1])\n",
    "\n",
    "        learning_rate = ExponentialDecay(initial_learning_rate=1, decay_rate=0.96, decay_steps=1)\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        self.model = Model(input_layer, output_layer)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        self.load_weights()\n",
    "\n",
    "    def load_weights(self):\n",
    "        try:\n",
    "            self.model.load_weights(self.weight_file)\n",
    "            print(\"Weights loaded from file.\")\n",
    "        except:\n",
    "            print(\"No weight file found.\")\n",
    "            pass\n",
    "\n",
    "    def save_weights(self):\n",
    "        self.model.save_weights(self.weight_file)\n",
    "\n",
    "    def preprocess(self, experience):\n",
    "        states = np.array([xp[0] for xp in experience])\n",
    "        actions = np.array([xp[1] for xp in experience])\n",
    "        rewards = np.array([xp[2] for xp in experience])\n",
    "        next_states = np.array([xp[3] for xp in experience])\n",
    "\n",
    "        neural_net_inputs = self.glue(states, actions)\n",
    "\n",
    "        q_values_go_left = self.model.predict(self.glue(next_states, np.zeros(len(next_states))))[:, 0]\n",
    "        q_values_go_right = self.model.predict(self.glue(next_states, np.ones(len(next_states))))[:, 0]\n",
    "\n",
    "        # this is the actual Q-learning policy!\n",
    "        neural_net_outputs = rewards + self.gamma * np.maximum(q_values_go_left, q_values_go_right)\n",
    "        \n",
    "        # This is when it the stick fell down\n",
    "        neural_net_outputs[-1] = self.worst_reward\n",
    "\n",
    "        return neural_net_inputs, neural_net_outputs\n",
    "\n",
    "    def get_action(self, state, eps=0):\n",
    "        take_rand_choice = np.random.choice(self.all_actions, p=[1.0 - eps, eps]) != 0\n",
    "        if take_rand_choice:\n",
    "            return np.random.choice(self.all_actions)\n",
    "        else:\n",
    "            inp = self.glue(np.array([state, state]), np.array([0, 1]))\n",
    "            pred = self.model.predict(inp)[:, 0]\n",
    "            return self.all_actions[np.where(pred == max(pred))][0]\n",
    "\n",
    "    def train(self, inputs, outputs, verbose=1):\n",
    "        self.model.evaluate(inputs, outputs, batch_size=len(inputs), verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd2261",
   "metadata": {},
   "source": [
    "## A look at the problem before we train\n",
    "Before we start to train the neural network, let us look at the problem and see how the untrained agent performs.\n",
    "\n",
    "We render multiple episodes, because they are very short prior to training.\n",
    "\n",
    "(A window should open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc22afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearner(weight_file=\"cart_pole\")\n",
    "\n",
    "def render_episode(agent):\n",
    "    step_no = 0\n",
    "    state = env.reset()\n",
    "    stick_fallen_down = False\n",
    "    \n",
    "    while not stick_fallen_down:\n",
    "        env.render()\n",
    "        action = agent.get_action(state)\n",
    "        state, reward, stick_fallen_down, info = env.step(action)\n",
    "        step_no += 1\n",
    "        \n",
    "    env.close()\n",
    "        \n",
    "    print(\"Episode lasted\", step_no, \"steps.\")\n",
    "\n",
    "for i in range(10):\n",
    "    render_episode(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3c99f",
   "metadata": {},
   "source": [
    "Before we finally start the training loop, we define some helpful functions:\n",
    "* A function to perform one training step and save it\n",
    "* The randomness decay. We reset back to 100% moves if the QLearner fails to start \n",
    "learning.\n",
    "* A function getting the next action based upon the current state from the QLearner and saves it to the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a623ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps_to_reset = 20\n",
    "\n",
    "def train():\n",
    "    experience = xp_buffer.get_experience_batch()\n",
    "    inputs, outputs = agent.preprocess(experience)\n",
    "    agent.train(inputs=inputs, outputs=outputs, verbose=0)\n",
    "    \n",
    "def get_new_relative_randomness(relative_randomness, avg_steps):\n",
    "    if avg_steps < max_steps_to_reset:\n",
    "        return 1\n",
    "    else:\n",
    "        return round(relative_randomness * randomness_decay, 5)\n",
    "    \n",
    "def reset_agent_if_necessary(agent, avg_steps):\n",
    "    if avg_steps < max_steps_to_reset:\n",
    "        agent = QLearner(weight_file=\"cart_pole\")\n",
    "        \n",
    "def perform_action_and_save(state):\n",
    "    action = agent.get_action(state, eps=relative_randomness)\n",
    "    next_state, reward, stick_fallen_down, info = env.step(action)\n",
    "    xp_buffer.save_step(state, action, reward, next_state)\n",
    "    return (stick_fallen_down, next_state)\n",
    "\n",
    "def simulate_episode():\n",
    "    state = env.reset()\n",
    "    stick_fallen_down = False\n",
    "    step_no = 0\n",
    "    while not stick_fallen_down:\n",
    "        (stick_fallen_down, state) = perform_action_and_save(state)\n",
    "        step_no += 1\n",
    "    return step_no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003829c5",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "Finally we are ready, to train. We initialized the QLearner and proceed as follows.\n",
    "1. We loop over as many training episodes as allowed above\n",
    "2. In every loop we balance the stick for as long as possible using a combination of random moves and moves predicted by the QLearner. Then we save them to the buffer.\n",
    "3. Once we have some episodes in the buffer we start training the neural network on every cycle\n",
    "4. Every `100` episodes, we decrease the randomness of the actions used\n",
    "5. If the average number of steps every falls below a certain value, we empirically found that it's hard for the algorithm to improve, such that we reset and start again.\n",
    "6. If the average step number exceeds `200`, we consider the problem as solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1e153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_after_n_episodes = 100\n",
    "steps_for_problem_to_be_solved = 200\n",
    "episodes_to_average_over = 100\n",
    "minimum_buffer_size = 5\n",
    "status_fmt_str = \"{}: avg. steps in the last {} episodes: {}. relative share of random moves = {}\"\n",
    "\n",
    "step_no = 0\n",
    "for i in range(episodes_until_abortion):\n",
    "    step_no += simulate_episode()\n",
    "\n",
    "    if i > minimum_buffer_size:\n",
    "        train()\n",
    "\n",
    "    if i % checkpoint_after_n_episodes == 0 and i > 0:\n",
    "        avg_steps = step_no/episodes_to_average_over\n",
    "        print(status_fmt_str.format(i, episodes_to_average_over, avg_steps, relative_randomness))\n",
    "        relative_randomness = get_new_relative_randomness(relative_randomness, avg_steps)\n",
    "        reset_agent_if_necessary(agent, avg_steps)\n",
    "        step_no = 0\n",
    "\n",
    "    if step_no/episodes_to_average_over >= steps_for_problem_to_be_solved:\n",
    "        print(\"Problem is solved!\")\n",
    "        agent.save_weights()\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c607c16",
   "metadata": {},
   "source": [
    "Let us once again render a couple of episodes of the agent to get an idea how it performs now:\n",
    "\n",
    "(You can load the weights for the model I trained by uncommenting the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f90c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = QLearner(weight_file=\"cart_pole_demo\")\n",
    "for i in range(5):\n",
    "    render_episode(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3526343",
   "metadata": {},
   "source": [
    "## Possible improvements:\n",
    "* To keep the model simple, we have completely disregarded the velocity component of the cart, including into a more complex model is likely to further improve the performance\n",
    "* We used a simple exponential decay for the randomness. This had the drawback that we needed to set a very slow decay to be sure that the learning started at the start, while it could be faster later on. This could be either implementing by having an accelerated decay or by implementing an adaptive decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
